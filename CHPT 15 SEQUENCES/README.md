# 

15 Neural Networks for Sequences 493 
15.1 Introduction 493 
15.2 Recurrent neural networks (RNNs) 493 
15.2.1 Vec2Seq (sequence generation) 493 
15.2.2 Seq2Vec (sequence classification) 496 
15.2.3 Seq2Seq (sequence translation) 497 
15.2.4 Teacher forcing 499
15.2.5 Backpropagation through time 500 
15.2.6 Vanishing and exploding gradients 501 
15.2.7 Gating and long term memory 502 
15.2.8 Beam search 505
15.3 1d CNNs 506 
15.3.1 1d CNNs for sequence classification 506 
15.3.2 Causal 1d CNNs for sequence generation 507
15.4 Attention 508 
15.4.1 Attention as soft dictionary lookup 509 
15.4.2 Kernel regression as non-parametric attention 510 
15.4.3 Parametric attention 510 
15.4.4 Seq2Seq with attention 511 
15.4.5 Seq2vec with attention (text classification) 514 
15.4.6 Seq+Seq2Vec with attention (text pair classification) 514 
15.4.7 Soft vs hard attention 515
15.5 Transformers 516 
15.5.1 Self-attention 516 
15.5.2 Multi-headed attention 517 
15.5.3 Positional encoding 518 
15.5.4 Putting it all together 519 
15.5.5 Comparing transformers, CNNs and RNNs 521 
15.5.6 Transformers for images * 522 
15.5.7 Other transformer variants * 523
15.6 Efficient transformers * 523